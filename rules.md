---
layout: default
---

# Rules

* <p style='text-align: justify;'> Each submission will be ranked individually for each task, with the top 10 submissions with the highest score per task receiving scores from 10 to 1. The score of each task is calculated as the average score of the three models for the first track. For the second track, the score is the score of the submitted model. </p>

* <p style='text-align: justify;'> We will measure the models' throughput on a smartphone platform provided by the sponsor. Submissions will be ranked based on throughput. To emphasize the importance of inference speed, the score for the throughput task will be <b>doubled</b>, ranging from 20 to 2 for the top 10 submissions. To assist participants in improving the speed of their models, we will provide an easy-to-run pipeline to measure throughput on a GPU, where the throughput values are roughly scaled to those on the smartphone platform.</p>

* <p style='text-align: justify;'> We will measure the inference memory usage of all models. Submissions with models that exceed 12GB of memory usage for inference will be disqualified.</p>

* <p style='text-align: justify;'> The final rank of submissions will be determined by the sum of scores across all evaluation tasks including seven diverse tasks as shown in the Table below (100 scores in total), maximum 70 scores, one secret holdout task (maximum 10 scores), and the throughput measurement (maximum 20 scores). </p>

## Protocol

<p style='text-align: justify;'>
We will consider using and will conduct beta testing of fully fledged platforms such as Codalab or Huawei Cloud Competition for hosting the competition. Alternatively, we may use online forms on the website for registration and submissions. Participants will follow the outlined steps to register for the competition:

* <p style='text-align: justify;'>The participants register on the website by entering their team name, email address, and affiliations using a form. Then, they receive an ID code to identify their submissions. 

* <p style='text-align: justify;'>During registration process, the participant is asked to agree to abide by the rules of the competition.

* <p style='text-align: justify;'>The participant can download the starter kit and the dataset from given repository and download links and follow the Tutorial notebook to learn about the dataset format, data
readers, evaluation protocol and the details of the baseline models.

<p style='text-align: justify;'>
  
Registered participants will need to include the following in their submission:
  
* <p style='text-align: justify;'>Same ID code given after registration to identify their multiple submissions

* <p style='text-align: justify;'>A Poetry configuration to manage the participantâ€™s Python library dependencies in a deter-ministic way. We will give a template and tutorial to participants.

* <p style='text-align: justify;'>A brief description of their method will required for the winning submissions.

<p style='text-align: justify;'>
Please note that the above submission process might slightly change if we can test and successfully adopt of competition platform.

<p style='text-align: justify;'>
Tsssso prevent cheating, we will keep the test set confidential before, during and after the competition is finished. We will also review every potentially winning submission to check for cheating.
